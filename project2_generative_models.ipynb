{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXmaPEJ2YrsOiy5gZo7zN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raycmarange/AML425_RAY/blob/main/project2_generative_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AIML425 - Assignment 2 (Generative Models)\n",
        "# Refactored solution with explicit SGD and comprehensive evaluations\n",
        "# Author: Ray Marange (original), refactored for completeness and robustness\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import kstest, chisquare\n",
        "\n",
        "# =========================\n",
        "# Reproducibility and device\n",
        "# =========================\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# =========================\n",
        "# Utilities\n",
        "# =========================\n",
        "def to_np(t):\n",
        "    return t.detach().cpu().numpy()\n",
        "\n",
        "def ensure_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "OUTDIR = \"outputs_assn2\"\n",
        "ensure_dir(OUTDIR)\n",
        "\n",
        "# =========================\n",
        "# Distribution samplers\n",
        "# =========================\n",
        "def gaussian_2d(batch_size):\n",
        "    return torch.randn(batch_size, 2)\n",
        "\n",
        "def uniform_2d(batch_size):\n",
        "    return torch.rand(batch_size, 2) - 0.5  # uniform on [-0.5, 0.5]^2\n",
        "\n",
        "def uniform_1d(batch_size):\n",
        "    return torch.rand(batch_size, 1) - 0.5  # uniform on [-0.5, 0.5]\n",
        "\n",
        "class EmpiricalSampler:\n",
        "    \"\"\"Wraps a dataset (N,d) tensor/array to return random mini-batches.\"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.as_tensor(data, dtype=torch.float32)\n",
        "        self.N = self.data.shape[0]\n",
        "    def __call__(self, batch_size):\n",
        "        idx = torch.randint(0, self.N, (batch_size,))\n",
        "        return self.data[idx]\n",
        "\n",
        "# =========================\n",
        "# Kernels and MMD\n",
        "# =========================\n",
        "def gaussian_kernel_multi(x, y, sigmas=(0.2, 0.5, 1.0, 2.0, 5.0)):\n",
        "    \"\"\"\n",
        "    Multi-kernel Gaussian RBF: average of RBFs with multiple bandwidths.\n",
        "    More stable across scales than a single sigma.\n",
        "    \"\"\"\n",
        "    x2 = (x**2).sum(dim=1, keepdim=True)\n",
        "    y2 = (y**2).sum(dim=1, keepdim=True)\n",
        "    dist = x2 + y2.T - 2 * (x @ y.T)\n",
        "    K = 0\n",
        "    for s in sigmas:\n",
        "        K = K + torch.exp(-dist / (2 * s**2))\n",
        "    return K / len(sigmas)\n",
        "\n",
        "def mmd2_unbiased(x, y, kernel=gaussian_kernel_multi):\n",
        "    \"\"\"\n",
        "    Unbiased MMD^2 estimator (U-statistic).\n",
        "    \"\"\"\n",
        "    n = x.shape[0]\n",
        "    assert y.shape[0] == n, \"Use equal batch sizes for unbiased MMD.\"\n",
        "    Kxx = kernel(x, x)\n",
        "    Kyy = kernel(y, y)\n",
        "    Kxy = kernel(x, y)\n",
        "    # Remove diagonals for unbiased estimator\n",
        "    sum_Kxx = (Kxx.sum() - Kxx.diag().sum()) / (n * (n - 1))\n",
        "    sum_Kyy = (Kyy.sum() - Kyy.diag().sum()) / (n * (n - 1))\n",
        "    sum_Kxy = Kxy.mean()\n",
        "    return sum_Kxx + sum_Kyy - 2 * sum_Kxy\n",
        "\n",
        "# =========================\n",
        "# Networks\n",
        "# =========================\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=128, depth=3, activation='relu'):\n",
        "        super().__init__()\n",
        "        acts = {\n",
        "            'relu': nn.ReLU,\n",
        "            'gelu': nn.GELU,\n",
        "            'tanh': nn.Tanh,\n",
        "            'leakyrelu': lambda: nn.LeakyReLU(0.2)\n",
        "        }\n",
        "        Act = acts[activation.lower()]\n",
        "        layers = []\n",
        "        d_in = input_dim\n",
        "        for _ in range(depth - 1):\n",
        "            layers += [nn.Linear(d_in, hidden_dim), Act()]\n",
        "            d_in = hidden_dim\n",
        "        layers += [nn.Linear(d_in, output_dim)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# =========================\n",
        "# Training (explicit SGD)\n",
        "# =========================\n",
        "def train_generator(\n",
        "    source_sampler, target_sampler, input_dim, output_dim,\n",
        "    epochs=5000, batch_size=512, lr=0.01,\n",
        "    reg_lambda=0.0, reg_type=None,\n",
        "    hidden_dim=128, depth=3, activation='relu',\n",
        "    kernel=gaussian_kernel_multi, log_every=500, name=\"net\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a generator with explicit SGD updates (no optimizer.step()).\n",
        "    Returns model, weight history, loss history.\n",
        "    \"\"\"\n",
        "    net = Generator(input_dim, output_dim, hidden_dim, depth, activation).to(device)\n",
        "    weight_history, loss_history = [], []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        net.train()\n",
        "        # Sample source and target mini-batches\n",
        "        x = source_sampler(batch_size).to(device)\n",
        "        y = target_sampler(batch_size).to(device)\n",
        "\n",
        "        # Forward\n",
        "        y_hat = net(x)\n",
        "\n",
        "        # MMD^2 loss\n",
        "        loss = mmd2_unbiased(y_hat, y, kernel=kernel)\n",
        "\n",
        "        # Regularization\n",
        "        if reg_type == 'l1':\n",
        "            l1 = sum(p.abs().sum() for p in net.parameters())\n",
        "            loss = loss + reg_lambda * l1\n",
        "        elif reg_type == 'l2':\n",
        "            l2 = sum((p**2).sum() for p in net.parameters())\n",
        "            loss = loss + reg_lambda * l2\n",
        "\n",
        "        # Backprop\n",
        "        for p in net.parameters():\n",
        "            if p.grad is not None:\n",
        "                p.grad.detach_()\n",
        "                p.grad.zero_()\n",
        "        loss.backward()\n",
        "\n",
        "        # Explicit SGD update\n",
        "        with torch.no_grad():\n",
        "            for p in net.parameters():\n",
        "                p.data -= lr * p.grad\n",
        "\n",
        "        # Logging\n",
        "        loss_history.append(float(loss.detach().cpu()))\n",
        "        if epoch % log_every == 0 or epoch == 1:\n",
        "            w = torch.cat([p.data.view(-1) for p in net.parameters()]).detach().cpu().numpy()\n",
        "            weight_history.append(w)\n",
        "            print(f\"[{name}] epoch {epoch:5d}/{epochs}  MMD2={loss_history[-1]:.6f}\")\n",
        "\n",
        "    return net, np.array(weight_history, dtype=np.float32), np.array(loss_history, dtype=np.float32)\n",
        "\n",
        "# =========================\n",
        "# Evaluation helpers\n",
        "# =========================\n",
        "def eval_uniform_2d(samples_np, grid_n=10):\n",
        "    \"\"\"\n",
        "    Diagnostics for uniformity on [-0.5,0.5]^2:\n",
        "    - Marginal KS tests\n",
        "    - Correlation\n",
        "    - 2D chi-square on a grid\n",
        "    \"\"\"\n",
        "    x = samples_np[:, 0]\n",
        "    y = samples_np[:, 1]\n",
        "\n",
        "    # Marginal KS against Uniform(-0.5, 0.5)\n",
        "    px = kstest(x, 'uniform', args=(-0.5, 1.0)).pvalue\n",
        "    py = kstest(y, 'uniform', args=(-0.5, 1.0)).pvalue\n",
        "\n",
        "    # Independence (Pearson correlation ~ 0)\n",
        "    corr = np.corrcoef(x, y)[0, 1]\n",
        "\n",
        "    # 2D chi-square\n",
        "    H, _, _ = np.histogram2d(x, y, bins=grid_n, range=[[-0.5, 0.5], [-0.5, 0.5]])\n",
        "    H = H.astype(np.float64)\n",
        "    expected = H.sum() / (grid_n * grid_n)\n",
        "    chi2_stat = ((H - expected) ** 2 / (expected + 1e-12)).sum()\n",
        "    dof = grid_n * grid_n - 1\n",
        "    # Approx p-value via gamma approximation of chi-square (scipy.chi2.sf if desired)\n",
        "    # We use chisquare on flattened counts with a flat expected:\n",
        "    chi2_p = chisquare(H.ravel(), f_exp=np.full(H.size, expected)).pvalue\n",
        "\n",
        "    return {\n",
        "        \"ks_x_p\": px, \"ks_y_p\": py,\n",
        "        \"corr_xy\": corr,\n",
        "        \"chi2_stat\": float(chi2_stat), \"chi2_p\": float(chi2_p)\n",
        "    }\n",
        "\n",
        "def eval_gaussian_2d(samples_np):\n",
        "    \"\"\"\n",
        "    Diagnostics for N(0, I) in 2D:\n",
        "    - Mean near 0, covariance near I\n",
        "    - Marginal normality (KS is weak for normals but indicative)\n",
        "    \"\"\"\n",
        "    mu = samples_np.mean(axis=0)\n",
        "    cov = np.cov(samples_np.T)\n",
        "    # Marginal KS tests against N(0,1)\n",
        "    px = kstest(samples_np[:, 0], 'norm').pvalue\n",
        "    py = kstest(samples_np[:, 1], 'norm').pvalue\n",
        "    return {\n",
        "        \"mean\": mu,\n",
        "        \"cov\": cov,\n",
        "        \"ks_x_p\": float(px),\n",
        "        \"ks_y_p\": float(py),\n",
        "        \"trace_cov\": float(np.trace(cov)),\n",
        "        \"det_cov\": float(np.linalg.det(cov))\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# 2.1: f1: Gaussian -> Uniform([-0.5,0.5]^2)\n",
        "# =========================\n",
        "print(\"Training f1: Gaussian -> Uniform...\")\n",
        "f1, w_f1_hist, f1_losses = train_generator(\n",
        "    source_sampler=gaussian_2d,\n",
        "    target_sampler=uniform_2d,\n",
        "    input_dim=2, output_dim=2,\n",
        "    epochs=10000, batch_size=512, lr=0.01,\n",
        "    reg_lambda=0.0, reg_type=None,\n",
        "    hidden_dim=128, depth=3, activation='relu',\n",
        "    name=\"f1\"\n",
        ")\n",
        "\n",
        "# Evaluate and visualize\n",
        "with torch.no_grad():\n",
        "    z = gaussian_2d(4000).to(device)\n",
        "    y_gen = f1(z).detach().cpu()\n",
        "y_eval = eval_uniform_2d(to_np(y_gen))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(to_np(z)[:,0], to_np(z)[:,1], s=6, alpha=0.4)\n",
        "plt.title(\"Source: Gaussian (2D)\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(to_np(y_gen)[:,0], to_np(y_gen)[:,1], s=6, alpha=0.4)\n",
        "plt.title(\"Generated: Uniform [-0.5,0.5]^2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"2_1_gaussian_to_uniform_scatter.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(f1_losses)\n",
        "plt.title(\"f1 training: MMD^2 vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"MMD^2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"2_1_f1_loss_curve.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(f\"2.1 diagnostics: KS p(x)={y_eval['ks_x_p']:.4f}, KS p(y)={y_eval['ks_y_p']:.4f}, \"\n",
        "      f\"corr={y_eval['corr_xy']:.4f}, chi2 p={y_eval['chi2_p']:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# 2.2: f2: Uniform -> Gaussian (inverse map)\n",
        "# =========================\n",
        "print(\"\\nTraining f2: Uniform -> Gaussian...\")\n",
        "f2, w_f2_hist, f2_losses = train_generator(\n",
        "    source_sampler=uniform_2d,\n",
        "    target_sampler=gaussian_2d,\n",
        "    input_dim=2, output_dim=2,\n",
        "    epochs=8000, batch_size=512, lr=0.01,\n",
        "    reg_lambda=0.0, reg_type=None,\n",
        "    hidden_dim=128, depth=3, activation='relu',\n",
        "    name=\"f2\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    u = uniform_2d(4000).to(device)\n",
        "    z_gen = f2(u).detach().cpu()\n",
        "z_eval = eval_gaussian_2d(to_np(z_gen))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(to_np(u)[:,0], to_np(u)[:,1], s=6, alpha=0.4)\n",
        "plt.title(\"Source: Uniform [-0.5,0.5]^2\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(to_np(z_gen)[:,0], to_np(z_gen)[:,1], s=6, alpha=0.4)\n",
        "plt.title(\"Generated: Gaussian (2D)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"2_2_uniform_to_gaussian_scatter.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(f2_losses)\n",
        "plt.title(\"f2 training: MMD^2 vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"MMD^2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"2_2_f2_loss_curve.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# Cycle consistency (sanity)\n",
        "with torch.no_grad():\n",
        "    z0 = gaussian_2d(3000).to(device)\n",
        "    y1 = f1(z0)\n",
        "    z_rec = f2(y1)\n",
        "    mse_cycle = torch.mean((z0 - z_rec)**2).item()\n",
        "print(f\"2.2 diagnostics: mean={z_eval['mean']}, trace(cov)={z_eval['trace_cov']:.3f}, \"\n",
        "      f\"det(cov)={z_eval['det_cov']:.3f}, KS p(x)={z_eval['ks_x_p']:.4f}, KS p(y)={z_eval['ks_y_p']:.4f}, \"\n",
        "      f\"cycle MSE={mse_cycle:.6f}\")\n",
        "\n",
        "# =========================\n",
        "# 2.3: Regularization analysis for f1\n",
        "# =========================\n",
        "print(\"\\n2.3: Regularization analysis on f1...\")\n",
        "reg_grid = {\n",
        "    \"none_0.0\": (None, 0.0),\n",
        "    \"l1_0.001\": ('l1', 0.001),\n",
        "    \"l1_0.01\":  ('l1', 0.01),\n",
        "    \"l1_0.1\":   ('l1', 0.1),\n",
        "    \"l2_0.001\": ('l2', 0.001),\n",
        "    \"l2_0.01\":  ('l2', 0.01),\n",
        "    \"l2_0.1\":   ('l2', 0.1),\n",
        "}\n",
        "weight_snaps = {}\n",
        "summary_stats = {}\n",
        "\n",
        "for name, (rtype, lam) in reg_grid.items():\n",
        "    print(f\"Training f1 with reg={name} ...\")\n",
        "    _, w_hist, _ = train_generator(\n",
        "        source_sampler=gaussian_2d,\n",
        "        target_sampler=uniform_2d,\n",
        "        input_dim=2, output_dim=2,\n",
        "        epochs=5000, batch_size=512, lr=0.01,\n",
        "        reg_lambda=lam, reg_type=rtype,\n",
        "        hidden_dim=128, depth=3, activation='relu',\n",
        "        name=f\"f1_{name}\"\n",
        "    )\n",
        "    w_last = w_hist[-1] if len(w_hist) > 0 else None\n",
        "    weight_snaps[name] = w_last\n",
        "    if w_last is not None:\n",
        "        summary_stats[name] = {\n",
        "            \"mean\": float(w_last.mean()),\n",
        "            \"std\": float(w_last.std()),\n",
        "            \"frac_small\": float((np.abs(w_last) < 1e-3).mean())\n",
        "        }\n",
        "\n",
        "# Plot histograms of final weights\n",
        "cols = 3\n",
        "rows = math.ceil(len(weight_snaps) / cols)\n",
        "plt.figure(figsize=(5*cols, 3.5*rows))\n",
        "for i, (name, w) in enumerate(weight_snaps.items(), start=1):\n",
        "    plt.subplot(rows, cols, i)\n",
        "    if w is not None:\n",
        "        plt.hist(w, bins=60, alpha=0.8, color=\"#4472c4\")\n",
        "    plt.title(name)\n",
        "    plt.xlabel(\"weight\")\n",
        "    plt.ylabel(\"freq\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"2_3_regularization_weight_histograms.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(\"2.3 summary stats (mean, std, frac(|w|<1e-3)):\")\n",
        "for k, v in summary_stats.items():\n",
        "    print(f\"  {k:10s} -> mean={v['mean']:.4f}, std={v['std']:.4f}, frac_small={v['frac_small']:.3f}\")\n",
        "\n",
        "# =========================\n",
        "# 2.4: f3: 1D uniform -> 2D Gaussian (deterministic)\n",
        "# =========================\n",
        "print(\"\\nTraining f3: 1D Uniform -> 2D Gaussian...\")\n",
        "f3, w_f3_hist, f3_losses = train_generator(\n",
        "    source_sampler=uniform_1d,\n",
        "    target_sampler=gaussian_2d,\n",
        "    input_dim=1, output_dim=2,\n",
        "    epochs=12000, batch_size=512, lr=0.01,\n",
        "    reg_lambda=0.0, reg_type=None,\n",
        "    hidden_dim=128, depth=4, activation='tanh',  # tanh can help smooth 1D mapping\n",
        "    name=\"f3\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    u_line = torch.linspace(-0.5, 0.5, 3000).view(-1,1).to(device)\n",
        "    z_from_line = f3(u_line).detach().cpu()\n",
        "    z_eval_f3 = eval_gaussian_2d(to_np(z_from_line))\n",
        "\n",
        "# Visualize mapping and distribution\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.scatter(to_np(u_line), np.zeros_like(to_np(u_line)), s=6, alpha=0.5)\n",
        "plt.title(\"1D Uniform Input (line)\")\n",
        "plt.subplot(1,3,2)\n",
        "plt.scatter(to_np(z_from_line)[:,0], to_np(z_from_line)[:,1],\n",
        "            c=to_np(u_line).ravel(), cmap='viridis', s=8)\n",
        "plt.colorbar(label='input u')\n",
        "plt.title(\"2D Output Colored by Input\")\n",
        "plt.subplot(1,3,3)\n",
        "plt.scatter(to_np(z_from_line)[:,0], to_np(z_from_line)[:,1], s=6, alpha=0.5)\n",
        "plt.title(\"Output Distribution (samples from f3(u))\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"2_4_1d_to_2d_mapping.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# Covariance eigenvalues (is it roughly full support in covariance sense?)\n",
        "cov_f3 = np.cov(to_np(z_from_line).T)\n",
        "eigs = np.linalg.eigvalsh(cov_f3)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(f3_losses)\n",
        "plt.title(\"f3 training: MMD^2 vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"MMD^2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR, \"2_4_f3_loss_curve.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(f\"2.4 diagnostics (deterministic): mean={z_eval_f3['mean']}, \"\n",
        "      f\"trace(cov)={z_eval_f3['trace_cov']:.3f}, det(cov)={z_eval_f3['det_cov']:.6f}, \"\n",
        "      f\"KS p(x)={z_eval_f3['ks_x_p']:.4f}, KS p(y)={z_eval_f3['ks_y_p']:.4f}, \"\n",
        "      f\"cov eigenvalues={eigs}\")\n",
        "\n",
        "print(\"\\nAll plots saved in:\", OUTDIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rrDfMxJXAy-",
        "outputId": "bbf282e8-09bd-4546-d7c2-3f15afce2efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training f1: Gaussian -> Uniform...\n",
            "[f1] epoch     1/10000  MMD2=0.120015\n",
            "[f1] epoch   500/10000  MMD2=-0.000145\n",
            "[f1] epoch  1000/10000  MMD2=-0.000278\n",
            "[f1] epoch  1500/10000  MMD2=0.000973\n",
            "[f1] epoch  2000/10000  MMD2=-0.000392\n",
            "[f1] epoch  2500/10000  MMD2=0.000000\n",
            "[f1] epoch  3000/10000  MMD2=-0.000061\n",
            "[f1] epoch  3500/10000  MMD2=0.001112\n",
            "[f1] epoch  4000/10000  MMD2=-0.000137\n",
            "[f1] epoch  4500/10000  MMD2=-0.000580\n",
            "[f1] epoch  5000/10000  MMD2=-0.000104\n",
            "[f1] epoch  5500/10000  MMD2=0.000086\n",
            "[f1] epoch  6000/10000  MMD2=0.000386\n",
            "[f1] epoch  6500/10000  MMD2=-0.000265\n",
            "[f1] epoch  7000/10000  MMD2=0.001418\n",
            "[f1] epoch  7500/10000  MMD2=0.000404\n",
            "[f1] epoch  8000/10000  MMD2=0.000533\n",
            "[f1] epoch  8500/10000  MMD2=-0.000335\n",
            "[f1] epoch  9000/10000  MMD2=-0.000491\n",
            "[f1] epoch  9500/10000  MMD2=-0.000003\n",
            "[f1] epoch 10000/10000  MMD2=-0.000011\n",
            "2.1 diagnostics: KS p(x)=0.0343, KS p(y)=0.0098, corr=0.0169, chi2 p=0.0000\n",
            "\n",
            "Training f2: Uniform -> Gaussian...\n",
            "[f2] epoch     1/8000  MMD2=0.401008\n",
            "[f2] epoch   500/8000  MMD2=0.020194\n",
            "[f2] epoch  1000/8000  MMD2=0.006407\n",
            "[f2] epoch  1500/8000  MMD2=0.006193\n",
            "[f2] epoch  2000/8000  MMD2=0.003311\n",
            "[f2] epoch  2500/8000  MMD2=0.002418\n",
            "[f2] epoch  3000/8000  MMD2=0.002867\n",
            "[f2] epoch  3500/8000  MMD2=0.002609\n",
            "[f2] epoch  4000/8000  MMD2=0.002681\n",
            "[f2] epoch  4500/8000  MMD2=0.003275\n",
            "[f2] epoch  5000/8000  MMD2=0.004858\n",
            "[f2] epoch  5500/8000  MMD2=0.002314\n",
            "[f2] epoch  6000/8000  MMD2=0.002141\n",
            "[f2] epoch  6500/8000  MMD2=0.002771\n",
            "[f2] epoch  7000/8000  MMD2=0.004057\n",
            "[f2] epoch  7500/8000  MMD2=0.004404\n",
            "[f2] epoch  8000/8000  MMD2=0.003639\n",
            "2.2 diagnostics: mean=[-0.02039544 -0.00209461], trace(cov)=1.646, det(cov)=0.677, KS p(x)=0.0000, KS p(y)=0.0000, cycle MSE=1.829204\n",
            "\n",
            "2.3: Regularization analysis on f1...\n",
            "Training f1 with reg=none_0.0 ...\n",
            "[f1_none_0.0] epoch     1/5000  MMD2=0.127207\n",
            "[f1_none_0.0] epoch   500/5000  MMD2=0.000238\n",
            "[f1_none_0.0] epoch  1000/5000  MMD2=0.001078\n",
            "[f1_none_0.0] epoch  1500/5000  MMD2=0.000276\n",
            "[f1_none_0.0] epoch  2000/5000  MMD2=-0.000416\n",
            "[f1_none_0.0] epoch  2500/5000  MMD2=-0.000346\n",
            "[f1_none_0.0] epoch  3000/5000  MMD2=-0.000585\n",
            "[f1_none_0.0] epoch  3500/5000  MMD2=-0.000197\n",
            "[f1_none_0.0] epoch  4000/5000  MMD2=-0.000421\n",
            "[f1_none_0.0] epoch  4500/5000  MMD2=-0.000010\n",
            "[f1_none_0.0] epoch  5000/5000  MMD2=0.000604\n",
            "Training f1 with reg=l1_0.001 ...\n",
            "[f1_l1_0.001] epoch     1/5000  MMD2=0.987993\n",
            "[f1_l1_0.001] epoch   500/5000  MMD2=0.788288\n",
            "[f1_l1_0.001] epoch  1000/5000  MMD2=0.709968\n",
            "[f1_l1_0.001] epoch  1500/5000  MMD2=0.636809\n",
            "[f1_l1_0.001] epoch  2000/5000  MMD2=0.568755\n",
            "[f1_l1_0.001] epoch  2500/5000  MMD2=0.505856\n",
            "[f1_l1_0.001] epoch  3000/5000  MMD2=0.446194\n",
            "[f1_l1_0.001] epoch  3500/5000  MMD2=0.393056\n",
            "[f1_l1_0.001] epoch  4000/5000  MMD2=0.344094\n",
            "[f1_l1_0.001] epoch  4500/5000  MMD2=0.300237\n",
            "[f1_l1_0.001] epoch  5000/5000  MMD2=0.262527\n",
            "Training f1 with reg=l1_0.01 ...\n",
            "[f1_l1_0.01] epoch     1/5000  MMD2=9.032032\n",
            "[f1_l1_0.01] epoch   500/5000  MMD2=2.764053\n",
            "[f1_l1_0.01] epoch  1000/5000  MMD2=1.244586\n",
            "[f1_l1_0.01] epoch  1500/5000  MMD2=1.054255\n",
            "[f1_l1_0.01] epoch  2000/5000  MMD2=0.894403\n",
            "[f1_l1_0.01] epoch  2500/5000  MMD2=0.758623\n",
            "[f1_l1_0.01] epoch  3000/5000  MMD2=0.638761\n",
            "[f1_l1_0.01] epoch  3500/5000  MMD2=0.530253\n",
            "[f1_l1_0.01] epoch  4000/5000  MMD2=0.436376\n",
            "[f1_l1_0.01] epoch  4500/5000  MMD2=0.351852\n",
            "[f1_l1_0.01] epoch  5000/5000  MMD2=0.283259\n",
            "Training f1 with reg=l1_0.1 ...\n",
            "[f1_l1_0.1] epoch     1/5000  MMD2=87.724625\n",
            "[f1_l1_0.1] epoch   500/5000  MMD2=2.408172\n",
            "[f1_l1_0.1] epoch  1000/5000  MMD2=1.022174\n",
            "[f1_l1_0.1] epoch  1500/5000  MMD2=1.026844\n",
            "[f1_l1_0.1] epoch  2000/5000  MMD2=1.024418\n",
            "[f1_l1_0.1] epoch  2500/5000  MMD2=1.015977\n",
            "[f1_l1_0.1] epoch  3000/5000  MMD2=1.017637\n",
            "[f1_l1_0.1] epoch  3500/5000  MMD2=1.033235\n",
            "[f1_l1_0.1] epoch  4000/5000  MMD2=1.019629\n",
            "[f1_l1_0.1] epoch  4500/5000  MMD2=1.005425\n",
            "[f1_l1_0.1] epoch  5000/5000  MMD2=1.017359\n",
            "Training f1 with reg=l2_0.001 ...\n",
            "[f1_l2_0.001] epoch     1/5000  MMD2=0.246372\n",
            "[f1_l2_0.001] epoch   500/5000  MMD2=0.106887\n",
            "[f1_l2_0.001] epoch  1000/5000  MMD2=0.107653\n",
            "[f1_l2_0.001] epoch  1500/5000  MMD2=0.102507\n",
            "[f1_l2_0.001] epoch  2000/5000  MMD2=0.100729\n",
            "[f1_l2_0.001] epoch  2500/5000  MMD2=0.098527\n",
            "[f1_l2_0.001] epoch  3000/5000  MMD2=0.097298\n",
            "[f1_l2_0.001] epoch  3500/5000  MMD2=0.094435\n",
            "[f1_l2_0.001] epoch  4000/5000  MMD2=0.093086\n",
            "[f1_l2_0.001] epoch  4500/5000  MMD2=0.090696\n",
            "[f1_l2_0.001] epoch  5000/5000  MMD2=0.088870\n",
            "Training f1 with reg=l2_0.01 ...\n",
            "[f1_l2_0.01] epoch     1/5000  MMD2=1.218981\n",
            "[f1_l2_0.01] epoch   500/5000  MMD2=0.874445\n",
            "[f1_l2_0.01] epoch  1000/5000  MMD2=0.717073\n",
            "[f1_l2_0.01] epoch  1500/5000  MMD2=0.589999\n",
            "[f1_l2_0.01] epoch  2000/5000  MMD2=0.487258\n",
            "[f1_l2_0.01] epoch  2500/5000  MMD2=0.401243\n",
            "[f1_l2_0.01] epoch  3000/5000  MMD2=0.333870\n",
            "[f1_l2_0.01] epoch  3500/5000  MMD2=0.276314\n",
            "[f1_l2_0.01] epoch  4000/5000  MMD2=0.230613\n",
            "[f1_l2_0.01] epoch  4500/5000  MMD2=0.194640\n",
            "[f1_l2_0.01] epoch  5000/5000  MMD2=0.165591\n",
            "Training f1 with reg=l2_0.1 ...\n",
            "[f1_l2_0.1] epoch     1/5000  MMD2=10.569101\n",
            "[f1_l2_0.1] epoch   500/5000  MMD2=1.573409\n",
            "[f1_l2_0.1] epoch  1000/5000  MMD2=0.426289\n",
            "[f1_l2_0.1] epoch  1500/5000  MMD2=0.260215\n",
            "[f1_l2_0.1] epoch  2000/5000  MMD2=0.227018\n",
            "[f1_l2_0.1] epoch  2500/5000  MMD2=0.216853\n",
            "[f1_l2_0.1] epoch  3000/5000  MMD2=0.212308\n",
            "[f1_l2_0.1] epoch  3500/5000  MMD2=0.166278\n",
            "[f1_l2_0.1] epoch  4000/5000  MMD2=0.167124\n",
            "[f1_l2_0.1] epoch  4500/5000  MMD2=0.169211\n",
            "[f1_l2_0.1] epoch  5000/5000  MMD2=0.165036\n",
            "2.3 summary stats (mean, std, frac(|w|<1e-3)):\n",
            "  none_0.0   -> mean=0.0000, std=0.0791, frac_small=0.010\n",
            "  l1_0.001   -> mean=0.0007, std=0.0554, frac_small=0.560\n",
            "  l1_0.01    -> mean=0.0003, std=0.0194, frac_small=0.989\n",
            "  l1_0.1     -> mean=0.0000, std=0.0006, frac_small=1.000\n",
            "  l2_0.001   -> mean=0.0005, std=0.0722, frac_small=0.012\n",
            "  l2_0.01    -> mean=0.0011, std=0.0308, frac_small=0.030\n",
            "  l2_0.1     -> mean=0.0000, std=0.0001, frac_small=0.997\n",
            "\n",
            "Training f3: 1D Uniform -> 2D Gaussian...\n",
            "[f3] epoch     1/12000  MMD2=0.418112\n",
            "[f3] epoch   500/12000  MMD2=0.070319\n",
            "[f3] epoch  1000/12000  MMD2=0.074429\n",
            "[f3] epoch  1500/12000  MMD2=0.069322\n",
            "[f3] epoch  2000/12000  MMD2=0.067882\n",
            "[f3] epoch  2500/12000  MMD2=0.064667\n",
            "[f3] epoch  3000/12000  MMD2=0.068633\n",
            "[f3] epoch  3500/12000  MMD2=0.072574\n",
            "[f3] epoch  4000/12000  MMD2=0.061227\n",
            "[f3] epoch  4500/12000  MMD2=0.068418\n",
            "[f3] epoch  5000/12000  MMD2=0.066569\n",
            "[f3] epoch  5500/12000  MMD2=0.064994\n",
            "[f3] epoch  6000/12000  MMD2=0.065875\n",
            "[f3] epoch  6500/12000  MMD2=0.066228\n",
            "[f3] epoch  7000/12000  MMD2=0.067025\n",
            "[f3] epoch  7500/12000  MMD2=0.065236\n",
            "[f3] epoch  8000/12000  MMD2=0.071256\n",
            "[f3] epoch  8500/12000  MMD2=0.069080\n",
            "[f3] epoch  9000/12000  MMD2=0.065975\n",
            "[f3] epoch  9500/12000  MMD2=0.060938\n",
            "[f3] epoch 10000/12000  MMD2=0.072130\n",
            "[f3] epoch 10500/12000  MMD2=0.068091\n",
            "[f3] epoch 11000/12000  MMD2=0.063503\n",
            "[f3] epoch 11500/12000  MMD2=0.067260\n",
            "[f3] epoch 12000/12000  MMD2=0.066438\n",
            "2.4 diagnostics (deterministic): mean=[-0.00717683 -0.01970837], trace(cov)=1.686, det(cov)=0.000062, KS p(x)=0.0000, KS p(y)=0.0000, cov eigenvalues=[3.66567092e-05 1.68644996e+00]\n",
            "\n",
            "All plots saved in: outputs_assn2\n"
          ]
        }
      ]
    }
  ]
}